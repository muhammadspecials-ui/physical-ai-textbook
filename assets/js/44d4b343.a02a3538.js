"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[905],{2083:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>c,metadata:()=>o,toc:()=>r});const o=JSON.parse('{"id":"module4/vision-language-action","title":"Module 4: Vision-Language-Action (VLA)","description":"The Convergence of LLMs and Robotics","source":"@site/docs/module4/vision-language-action.mdx","sourceDirName":"module4","slug":"/module4/vision-language-action","permalink":"/physical-ai-textbook/docs/module4/vision-language-action","draft":false,"unlisted":false,"editUrl":"https://github.com/muhammadspecials-ui/physical-ai-textbook/tree/main/docs/module4/vision-language-action.mdx","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122)","permalink":"/physical-ai-textbook/docs/module3/nvidia-isaac"}}');var t=i(4848),s=i(8453);const c={sidebar_position:1},a="Module 4: Vision-Language-Action (VLA)",l={},r=[{value:"The Convergence of LLMs and Robotics",id:"the-convergence-of-llms-and-robotics",level:2},{value:"Voice-to-Action Pipeline",id:"voice-to-action-pipeline",level:2},{value:"1. Speech Recognition with Whisper",id:"1-speech-recognition-with-whisper",level:3},{value:"2. Cognitive Planning with LLMs",id:"2-cognitive-planning-with-llms",level:3},{value:"3. Vision Processing",id:"3-vision-processing",level:3},{value:"4. Action Execution",id:"4-action-execution",level:3},{value:"Complete VLA System",id:"complete-vla-system",level:2},{value:"Capstone Project: The Autonomous Humanoid",id:"capstone-project-the-autonomous-humanoid",level:2},{value:"Congratulations! \ud83c\udf89",id:"congratulations-",level:2}];function d(e){const n={h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"module-4-vision-language-action-vla",children:"Module 4: Vision-Language-Action (VLA)"})}),"\n",(0,t.jsx)(n.h2,{id:"the-convergence-of-llms-and-robotics",children:"The Convergence of LLMs and Robotics"}),"\n",(0,t.jsxs)(n.p,{children:["Vision-Language-Action (VLA) models represent the cutting edge: AI systems that can ",(0,t.jsx)(n.strong,{children:"see"}),", ",(0,t.jsx)(n.strong,{children:"understand language"}),", and ",(0,t.jsx)(n.strong,{children:"take action"})," in the physical world."]}),"\n",(0,t.jsx)(n.h2,{id:"voice-to-action-pipeline",children:"Voice-to-Action Pipeline"}),"\n",(0,t.jsx)(n.h3,{id:"1-speech-recognition-with-whisper",children:"1. Speech Recognition with Whisper"}),"\n",(0,t.jsx)(n.h3,{id:"2-cognitive-planning-with-llms",children:"2. Cognitive Planning with LLMs"}),"\n",(0,t.jsx)(n.h3,{id:"3-vision-processing",children:"3. Vision Processing"}),"\n",(0,t.jsx)(n.h3,{id:"4-action-execution",children:"4. Action Execution"}),"\n",(0,t.jsx)(n.h2,{id:"complete-vla-system",children:"Complete VLA System"}),"\n",(0,t.jsx)(n.p,{children:"Integrate voice, vision, and action for autonomous robots that understand natural language commands."}),"\n",(0,t.jsx)(n.h2,{id:"capstone-project-the-autonomous-humanoid",children:"Capstone Project: The Autonomous Humanoid"}),"\n",(0,t.jsx)(n.p,{children:"Build a robot that:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"\u2705 Receives voice commands"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 Plans with GPT-4"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 Navigates with Nav2"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 Detects objects with YOLO"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 Manipulates in Isaac Sim"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"congratulations-",children:"Congratulations! \ud83c\udf89"}),"\n",(0,t.jsx)(n.p,{children:"You've completed the Physical AI & Humanoid Robotics course!"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Next Steps:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Build your capstone project"}),"\n",(0,t.jsx)(n.li,{children:"Contribute to open-source robotics"}),"\n",(0,t.jsx)(n.li,{children:"Join the Physical AI community"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"The future of robotics is in your hands! \ud83d\ude80\ud83e\udd16"})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>c,x:()=>a});var o=i(6540);const t={},s=o.createContext(t);function c(e){const n=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:c(e.components),o.createElement(s.Provider,{value:n},e.children)}}}]);